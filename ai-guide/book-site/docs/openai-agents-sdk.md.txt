## ðŸ› ï¸ Chapter 5 Content: OpenAI Agents SDK

**File to open:** `openai-agents-sdk.md` (inside the `docs` folder)

**Content to Copy and Paste:**

```markdown
---
sidebar_position: 5
title: OpenAI Agents SDK
---

# Chapter 5: OpenAI Agents SDK Integration

The final component ties the FastAPI API, the Qdrant retrieval system, and the large language model (LLM) together using the OpenAI SDK. The agents SDK facilitates the use of external tools and structured function calling.

## Installation

Ensure the correct OpenAI SDK is installed in your Python environment:

```bash
pip install openai-agents

RAG Function Definition (Tool)
To use RAG, we define a Python function that the LLM can call when it needs external knowledge. This function encapsulates the vector search logic:
# This function acts as the "tool" the AI will call
def retrieve_book_content(user_query: str) -> str:
    """
    Retrieves the most relevant content from the book's Qdrant store
    based on the user's question (user_query).
    """
    # 1. Convert user_query to a vector embedding (using an OpenAI embedding model)
    # 2. Query Qdrant with the vector to find the top K relevant text chunks
    # 3. Concatenate the text chunks and return them to the LLM
    
    # Placeholder return for demonstration:
    return "The Spec-Driven Methodology follows four phases: Specify, Plan, Tasks, and Implement."
Using the Function with the Chat Agent
We now instantiate the OpenAI client and register our RAG function as a tool the model can use:
from openai import OpenAI
import json

client = OpenAI()

# The system prompt guides the AI's behavior
SYSTEM_PROMPT = "You are an expert technical writer and helpful assistant. Use the retrieve_book_content tool to answer questions about 'The AI-Driven Spec-Kit Guide'."

def run_rag_chat(prompt):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt}
    ]

    response = client.chat.completions.create(
        model="gpt-4-turbo",  # Or another capable model
        messages=messages,
        tools=[{"type": "function", "function": retrieve_book_content}],
    )

    # Simplified function calling logic:
    # 1. Check if the model decided to call the function
    # 2. Call the function (retrieve_book_content) with the arguments the model provided
    # 3. Send the function result back to the model for the final answer
    
    return response.choices[0].message.content

# Example usage:
# print(run_rag_chat("What are the four phases of the Spec-Driven Methodology?"))